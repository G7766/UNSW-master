{
    "unsupervised learning": {
        "description": "Unsupervised learning is where you only have input data (X) and no corresponding output variables. \tThe goal for unsupervised learning is to model the underlying structure or distribution in the data in order to learn more about the data. \tThese are called unsupervised learning because unlike supervised learning above there is no correct answers and there is no teacher. \tAlgorithms are left to their own devises to discover and present the interesting structure in the data. \tUnsupervised learning problems can be further grouped into clustering and association problems.",
        "example": "1.k-means for clustering problems, 2.Apriori algorithm for association rule learning problems"
    },
    "supervised learning": {
        "description": "Supervised learning is where you have input variables (x) and an output variable (Y) and you use an algorithm to learn the mapping function from the input to the output. \tThe goal is to approximate the mapping function so well that when you have new input data (x) that you can predict the output variables (Y) for that data. \tIt is called supervised learning because the process of an algorithm learning from the training dataset can be thought of as a teacher supervising the learning process. \tWe know the correct answers, the algorithm iteratively makes predictions on the training data and is corrected by the teacher. Learning stops when the algorithm achieves an acceptable level of performance. \tSupervised learning problems can be further grouped into regression and classification problems.",
        "example": "1.Linear regression for regression problems, 2.Random forest for classification and regression problems, 3. Support vector machines for classification problems"
    },
    "k-means": {
        "how to code": "Step 1: Start with some initial cluster centers (k random points) \n\tStep 2: Iterate: \n\t\tAssign/cluster each example to closest center\n\t\tRecalculate and change centers as the mean of the points in the cluster.\n\tStep 3: Stop when no points\u02bc assignments change",
        "properties": "\u2013 Guaranteed to converge in a finite number of iterations. \n\t\u2013 Running time per iteration:\n\t\t1. Assign data points to closest cluster center O(KN) time.\n\t\t2. Change the cluster center to the average of its assigned points O(N)."
    },
    "logistic regression": {
        "description": "Logistic regression is the appropriate regression analysis to conduct when the dependent variable is dichotomous (binary). \tLike all regression analyses, the logistic regression is a predictive analysis. \tLogistic regression is used to describe data and to explain the relationship between one dependent binary variable and one or more nominal, ordinal, interval or ratio-level independent variables.",
        "properties": "Actually a technique for classification, not regression.\n\tA multidimensional feature space (features can be categorical or continuous).\n\tOutcome is discrete, not continuous.",
        "advantages": "\u2013 Makes no assumptions about distributions of classes in feature space.\n\t\u2013 Easily extended to multiple classes (multinomial regression).\n\t\u2013 Natural probabilistic view of class predictions.\n\t\u2013 Quick to train.\n\t\u2013 Very fast at classifying unknown records.\n\t\u2013 Good accuracy for many simple data sets.\n\t\u2013 Resistant to overfitting.\n\t\u2013 Can interpret model coefficients as indicators of feature importance.",
        "disadvantages": "Linear decision boundary"
    },
    "k-nearest neighbours": {
        "description": "Essentially, given a query item: Find k closest matches in a labeled dataset. \tBasic idea: If it walks like a duck, quacks like a duck, then it\u2019s probably a duck.",
        "properties": "\u2013 Part of instance based learning (lazy learning).\n\t\u2013 Non-parametric: makes no assumptions about the probability distribution the examples come from.\n\t\u2013 Does not assume data is linearly separable.\n\t\u2013 Derives decision rule directly from training data.\n\t\u2013 No information discarded: exceptional and low frequency training instances are available for prediction.",
        "how to code": "Requires three inputs:\n\t\t1. The set of stored samples.\n\t\t2. Distance metric to compute distance between samples.\n\t\t3. The value of k, the number of nearest neighbors (NN) to retrieve.\n\n\tTo classify unknown record:\n\t\t1. Compute distance to Unknown record other training records.\n\t\t2. Identify k NN\n\t\t3. Use class labels of NN to determine the class label of unknown record (e.g., by taking majority vote) Or to put simply, \tto classify a new input vector x, examine the k-closest training data points to x and assign the object to the most frequently occurring class.",
        "advantages": "\u2013 Training is very fast.\n\t\u2013 Learn complex target functions.\n\t\u2013 Do not lose information.",
        "disadvantages": "\u2013 Slow at query. Complexity: O(kdN).\n\t\u2013 Required amount of training data increases exponentially with dimension.\n\t\u2013 Must store all training data.\n\t\u2013 Easily fooled by irrelevant attributes (add noise to distance measure).\n\t\u2013 Sensitive to mis-labeled data and scales of attributes."
    },
    "decision trees": {
        "description": "Decision Trees are a type of Supervised Machine Learning (that is you explain what the input is and what the corresponding output is in the training data) where the data is continuously split according to a certain parameter. \tThe tree can be explained by two entities, namely decision nodes and leaves. The leaves are the decisions or the final outcomes. And the decision nodes are where the data is split.",
        "example": "1.Classification trees, 2.Regression trees.",
        "how to code": "Base Case (do not split a node):\n\t\t1. if all matching records have the same output value.\n\t\t2. if none of the attributes can create multiple nonempty children (run out of questions!)\n\n\tRecursive:\n\t\t1. Select the \u201cbest\u201d variable (based on Information Gain, Gini Index), and generate child nodes: One for each possible value.\n\t\t2. Partition samples using the possible values, and assign these subsets of samples to the child nodes.\n\t\t3. Repeat for each child node until all samples associated with a node that are either all positive or all negative.",
        "when to consider": "\u2013 Instances describable by attribute-value pairs.\n\t\u2013 Target function is discrete valued.\n\t\u2013 Disjunctive hypothesis may be required.\n\t\u2013 Possibly noisy training data.\n\t\u2013 Missing attribute values.",
        "properties": "Problem: exponentially large trees (overfitting).\n\tSimpler is better and avoids overfitting.\n\tOtherways of avoiding overfitting are: Stop growing when split not statistically significant, grow full tree, then post-prune and cross validation."
    }
}