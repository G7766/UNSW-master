{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Learning - implementing elements of neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Last revision: Fri Jul  5 18:34:09 AEST 2019_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab we will expand on some of the concepts of neural learning, starting with the perceptron. Initially we understand the representational capacity of a perceptron, then how to implement learning for elementary Boolean functions, i.e., concept learning, and look at a perceptron learning a linear classifier on a real-world dataset.\n",
    "\n",
    "The remainder of the lab goes into some \"hands-on\" aspects of supervised learning for neural networks, based on the multi-layer perceptron trained by error back-propagation. \n",
    "There are only questions as such in the first section, a review of perceptrons. For the second part on the multi-layer perceprton you are just supposed to step through the cells, running the code, understanding why it is doing what it does, and possibly adding your own cells to experiment.\n",
    "\n",
    "This code is for explanatory purposes only â€“ for real neural networks you would use one of the many code libraries that exist. \n",
    "\n",
    "**Note: this notebook has only been tested using Python 3.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acknowledgement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The perceptron implementation for this lab is based on the presentation and code in Chapter 3 of \"Machine Learning\" by Stephen Marsland, CRC Press, 2015. \n",
    "\n",
    "The multi-layer perceptron part of the lab is based on the presentation and code accompanying Chapter 18 of \"Data Science from Scratch\" by Joel Grus, O'Reilly Media, 2015 (all the code for the book is available [here](http://github.com/joelgrus/data-science-from-scratch))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) Linear classification with the Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab we will use a slight variant on the notation and setup\n",
    "used in the lectures.\n",
    "These changes are not going to affect the capabilities of the perceptron.\n",
    "\n",
    "For a given set of $m$ inputs, the first stage of the computation is when the perceptron  multiplies each of the input values with its corresponding weight and adds these together:\n",
    "\n",
    "$$ h = \\sum_{i}^{m} w_{i} x_{i} $$\n",
    "\n",
    "The second stage is to apply the thresholding output rule or activation function of the perceptron to produce the classification output.\n",
    "\n",
    "For this lab we will slightly change the activation function to map to either $0$ or $1$ rather than the $-1$ or $+1$ we had in the lecture notes.\n",
    "\n",
    "The value set for the bias or threshold input will also be changed from $1$ to $-1$.\n",
    "\n",
    "$$ o = g(h) = \\left\\{\n",
    "                \\begin{array}{lll}\n",
    "                        1 & \\mbox{if}           & h > 0 \\\\\n",
    "                        0 & \\mbox{otherwise if} & h \\leq 0 \\\\\n",
    "                \\end{array}\n",
    "              \\right. $$\n",
    "\n",
    "Let's go ahead and implement a Perceptron in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representing simple Boolean functions as a linear classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first look at modelling a simple two-input Boolean function as linear classifier. This is a Perceptron WITHOUT any learning! To get started we will use the OR function, for which the truth table will be familiar to you all. Note that you will need to pick some weights for the function to output the correct values given the input. There are many possible values that could do the job. Also, remember to take care with the dimension of the weight vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'w' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-0e245108b544>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# loop over the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m# this is the bias weight and input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mh\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'w' is not defined"
     ]
    }
   ],
   "source": [
    "# set up the data, i.e., all the cases in the truth table \n",
    "x=[[0,0],[0,1],[1,0],[1,1]]\n",
    "y=[0,1,1,1]\n",
    "# number of data points\n",
    "n=4\n",
    "# number of inputs to the perceptron\n",
    "m=3\n",
    "# what weights should be assigned to correctly compute the OR function ?\n",
    "# fill in your weights here by assigning a weight vector to w\n",
    "# e.g., w=[-0.09,0.14,0.01]\n",
    "# how can you come up with a suitable set of weights ?\n",
    "# loop over the data\n",
    "for i in range(n):\n",
    "    h=w[0]*(-1)# this is the bias weight and input\n",
    "    for j in range(1,m):\n",
    "        h+=w[j]*x[i][j-1]\n",
    "    if(h>0):\n",
    "        output=1\n",
    "    else:\n",
    "        output=0\n",
    "    print('For Input', x[i], 'with Class', y[i], 'Predict ', output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now change your code to model the AND function (again restricted to two inputs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the data, i.e., all the cases in the truth table \n",
    "x=[[0,0],[0,1],[1,0],[1,1]]\n",
    "y=[0,0,0,1]\n",
    "# number of data points\n",
    "n=4\n",
    "# number of inputs to the perceptron\n",
    "m=3\n",
    "# what weights should be assigned to correctly compute the AND function ?\n",
    "# fill in your weights here by assigning a weight vector to w\n",
    "# how can you come up with a suitable set of weights ?\n",
    "# loop over the data\n",
    "for i in range(n):\n",
    "    h=w[0]*(-1)# this is the bias weight and input\n",
    "    for j in range(1,m):\n",
    "        h+=w[j]*x[i][j-1]\n",
    "    if(h>0):\n",
    "        output=1\n",
    "    else:\n",
    "        output=0\n",
    "    print('For Input', x[i], 'with Class', y[i], 'Predict ', output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing the data structures for machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got right down to the details of how a linear classifier works. Now this being a perceptron, you probably recall that rather than using a fixed set of weights to do the prediction each time, there is a simple training rule that updates the weights on the basis of discrepancies between the classifier's prediction on the data and the actual class. So we could extend our previous code to implement that training rule, but the code is a little fiddly and you're probably thinking there should be a simpler way to do this. If so, you are correct, but it is based on moving towards coding with matrix and vector operations, rather than directly using Python arrays. To do this we need to import the NumPy library (there is a tutorial at: <href <a>https://docs.scipy.org/doc/numpy-dev/user/quickstart.html</a>>)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, when we need to predict a class for an instance $\\mathbf{x}$ given the current weights $\\mathbf{w}$ we can use the inner product operation $\\mathbf{x} \\cdot \\mathbf{w}$. To get this functionality using NumPy we just do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x=np.array([0,1,1])\n",
    "w=np.array([0.02,0.03,0.03])\n",
    "\n",
    "h=np.dot(x,w)\n",
    "print(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But wait, there's more! Since $\\mathbf{x}$ and $\\mathbf{w}$ are both actually matrices, the same operation will enable us to apply the inner product of the weight vector $\\mathbf{w}$ to ALL the data instances at once. In this case we write the matrix of data instances $\\mathbf{X}$. Just note that we need to take care that the data matrix and weight vector are properly initialised to make this operation work correctly. Now the code for predicting the class values of all of our data given the weight vector is as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Data set with class values in last column\n",
    "dataset = np.array([[0,0,0],[0,1,1],[1,0,1],[1,1,1]]) # OR function\n",
    "X=dataset[:,0:2]\n",
    "y = dataset[:,2:]\n",
    "# Note: the bias weight is now the last!\n",
    "w = np.array([[0.03],[0.03],[0.02]])\n",
    "# Add the values for the bias weights (-1) to the data matrix\n",
    "nData = np.shape(X)[0]\n",
    "X = np.concatenate((X,-np.ones((nData,1))),axis=1)\n",
    "# get the value of the activation function\n",
    "h = np.dot(X,w)\n",
    "yhat = np.where(h>0,1,0)\n",
    "err = yhat-y\n",
    "\n",
    "print('Activations:\\n', h)\n",
    "print('Predictions:\\n', yhat)\n",
    "print('Misclassifications\\n', err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code uses some more NumPy built-ins. Check the documentation to be sure you know what is going on. One of these, np.where(), is useful here. It takes 3 arguments and returns an array. The first argument is a predicate on an array that is either evaluates to true, returning the second argument at the corresponding index in the array or false, returning the third argument instead. Now see how you get on re-implementing the code to do the prediction for the two-input Boolean AND function, as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Data set with class values in last column\n",
    "# dataset for AND function\n",
    "X=dataset[:,0:2]\n",
    "y = dataset[:,2:]\n",
    "# Note: the bias weight is now the last!\n",
    "# fill in your weights here by assigning a weight vector to w\n",
    "# Add the values for the bias weights (-1) to the data matrix\n",
    "nData = np.shape(X)[0]\n",
    "X = np.concatenate((X,-np.ones((nData,1))),axis=1)\n",
    "# get the value of the activation function\n",
    "h = np.dot(X,w)\n",
    "yhat = np.where(h>0,1,0)\n",
    "err = yhat-y\n",
    "\n",
    "print('Activations:\\n', h)\n",
    "print('Predictions:\\n', yhat)\n",
    "print('Misclassifications\\n', err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding in weight updates to make the learning work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have spent some time just getting the weights and data in the right vector-matrix format to be able to do the prediction. What else do we need to get this thing to learn ?\n",
    "\n",
    "One thing we will need is some random initialisation for the weight vector. What sort of values would be appropriate for this initialisation?\n",
    "\n",
    "The initialisation will be done using a NumPy built-in. Note that we need weights for each of the inputs \"nIn\", plus one for the bias. Also, the \"nOut\" parameter is just a placeholder in case you want your Perceptron to predict more that one output at a time. Here we will just use one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nIn = 2    # still working with 2-input Boolean functions\n",
    "nOut = 1   # so a true/false classification output\n",
    "w = np.random.rand(nIn+1,nOut)*0.1-0.05 # Check: does this return a column vector?\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other main thing we need is to see how the Perceptron training rule is implemented to update the weights for each attribute given all the information in the data matrix plus the misclassifications. Note that this implementation is a batch version, unlike the version in the lecture notes which is incremental. Both approaches have their place. Here we go for simplicity of implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What must the inner dimensions of the matrix multiplcation be for the weight update ? Check with the lecture notes to see what terms we will need. Recall that the augmented data matrix has $m+1$ columns, where $m$ is the number of inputs. However, the misclassifications, or errors, are of dimensionality $n$, because there is potentially one misclassification for every example in the dataset. What has to happen ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correct: you need to transpose the augmented data matrix to ensure the inner dimensions match (they both must be of size $n$). Check you are sure before inspecting the code (it's just a one-liner). Here the parameter \"eta\" is the learning rate $\\eta$, which for this code is set to $0.25$. Once more $\\hat{y} - y$ will be our misclassification vector. Can you see why the updated weight vector $w$ has the values it does ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta=0.25\n",
    "w -= eta*np.dot(np.transpose(X),yhat-y)# this is it - learning in one line of code!\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can put it all together. Note that we need to set an upper limit for the number of iterations (T). Play with this code and run it as above for our Boolean functions. See what happens to the weights for \"OR\". Does the Perceptron learn this function? Now try \"AND\". Then try \"XOR\" (exclusive or). Now go back and experiment with the learning rate. Does anything change ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "\n",
    "# Dataset with class values in last column\n",
    "#dataset = np.array([[0,0,0],[0,1,1],[1,0,1],[1,1,1]])   # OR function\n",
    "# dataset for AND function\n",
    "# dataset for XOR function\n",
    "X = dataset[:,0:2]\n",
    "y = dataset[:,2:]\n",
    "nIn = np.shape(X)[1]    # no. of columns of data matrix\n",
    "nOut = np.shape(y)[1]   # no. of columns of class values -- just 1 here\n",
    "nData = np.shape(X)[0]  # no. of rows of data matrix\n",
    "w = np.random.rand(nIn+1,nOut)*0.1-0.05\n",
    "X = np.concatenate((X,-np.ones((nData,1))),axis=1)\n",
    "eta=0.25\n",
    "T=20\n",
    "# Train for T iterations\n",
    "for t in range(T):\n",
    "        # Predict outputs given current weights\n",
    "        h = np.dot(X,w)\n",
    "        yhat = np.where(h>0,1,0)\n",
    "        # Update weights for all incorrect classifications\n",
    "        w -= eta*np.dot(np.transpose(X),yhat-y)\n",
    "        # Output current performance\n",
    "        errors=yhat-y\n",
    "        perrors=((nData - np.sum(np.where(errors==0,1,0)))/nData)\n",
    "        # print(perrors, 'is Error on iteration:', t)\n",
    "        print('Iteration:', t, ' Error:', perrors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron training on real data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, try this out on a real dataset, the standard diabetes dataset. You can download this from within your program. The rest of your program should work the same. Replace the lines defining the dataset, X and y variables with the code below. Perhaps surprisingly this simple algorithm actually learns to classify, but unfortunately, this basic implementation of neural learning is not likely to find a very good model. It's also not clear if it converges. You might want to increase the number of iterations from 20. Also, you could try transforming the data, for example, by making all attribute values lie in the same range. Search for methods of normalisation using the NumPy built-in functions \"np.mean()\" and \"np.var()\". For example, you could transform dataset ```X``` with this normalisation:\n",
    "```Z = (X - np.mean(X,axis = 0))/(np.var(X,axis = 0)**0.5)```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "# URL for a copy of the Pima Indians Diabetes dataset (UCI Machine Learning Repository)\n",
    "url = \"http://cse.unsw.edu.au/~mike/comp9417/data/uci_pima_indians_diabetes.csv\"\n",
    "# download the file\n",
    "raw_data = urllib.request.urlopen(url)\n",
    "# load the CSV file as a numpy matrix\n",
    "dataset = np.loadtxt(raw_data, delimiter=\",\")\n",
    "print(dataset.shape) # 8 attributes, 1 class, 768 examples\n",
    "X = dataset[:,0:8]\n",
    "y = dataset[:,8:9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the full code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "\n",
    "import urllib\n",
    "# URL for a copy of the Pima Indians Diabetes dataset (UCI Machine Learning Repository)\n",
    "url = \"http://cse.unsw.edu.au/~mike/comp9417/data/uci_pima_indians_diabetes.csv\"\n",
    "# download the file\n",
    "raw_data = urllib.request.urlopen(url)\n",
    "# load the CSV file as a numpy matrix\n",
    "dataset = np.loadtxt(raw_data, delimiter=\",\")\n",
    "print(dataset.shape) \n",
    "X = dataset[:,0:8]\n",
    "y = dataset[:,8:9]\n",
    "\n",
    "nIn = np.shape(X)[1]    # no. of columns of data matrix\n",
    "nOut = np.shape(y)[1]   # no. of columns of class values -- just 1 here\n",
    "nData = np.shape(X)[0]  # no. of rows of data matrix\n",
    "w = np.random.rand(nIn+1,nOut)*0.1-0.05\n",
    "X = np.concatenate((X,-np.ones((nData,1))),axis=1)\n",
    "eta=0.25\n",
    "T=20\n",
    "# Train for T iterations\n",
    "for t in range(T):\n",
    "        # Predict outputs given current weights\n",
    "        h = np.dot(X,w)\n",
    "        yhat = np.where(h>0,1,0)\n",
    "        # Update weights for all incorrect classifications\n",
    "        w -= eta*np.dot(np.transpose(X),yhat-y)\n",
    "        # Output current performance\n",
    "        errors=yhat-y\n",
    "        perrors=((nData - np.sum(np.where(errors==0,1,0)))/nData)\n",
    "        # print(perrors, 'is Error on iteration:', t)\n",
    "        print('Iteration:', t, ' Error:', perrors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, this code is just Python code so you can use this for further experimentation. Of course, it can be improved a lot!  A good starting point would be the NumPy documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) Implementing a Multi-layer Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although real-world applications of neural networks are typically based on one of the many special-purpose libraries (such as TensorFlow, PyTorch, CNTK, etc.) it is possible and instructive to implement at least a basic neural network just using standard Python libraries. We start by implementing some key functions and concepts for a multi-layer neural network. Before coding the fully connected multi-layer neural network, let us code some basic functions needed for  the multi-layer neural network. We will need several libraries later so it is easiest to import them first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from functools import partial\n",
    "import math, random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  What is the sigmoid function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    # To do\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the derivative of the sigmoid function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_der(x):\n",
    "    # To do\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the output function for neurons?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neuron_output(w,x,b):\n",
    "    # To do\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the softmax function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(y):\n",
    "    # To do\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to initialise the network weights?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_weight(input_dim,output_dim,hid_layers):\n",
    "    number_NN = hid_layers+[output_dim]\n",
    "\n",
    "    last_neural_number = input_dim\n",
    "    weight_list,bias_list = [],[]\n",
    "\n",
    "    for current_neural_number in number_NN:\n",
    "        # To do: code up some method to initialize weights and uncomment the following 2 lines \n",
    "        # current_weights = \n",
    "        # current_bias = \n",
    "\n",
    "        last_neural_number = current_neural_number\n",
    "\n",
    "        weight_list.append(current_weights)\n",
    "        bias_list.append(current_bias)\n",
    "\n",
    "    return weight_list,bias_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many functions did you manage to implement?\n",
    "\n",
    "You should have been able to think of code for most of these functions from your knowledge of neural networks. If you did manage to get some code, great: below there is a reference implementation of a multilayer perceptron in which most of your functions should work if you added them.\n",
    "\n",
    "To test this implementation we will use a toy dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example application: simplified hand-written digit classification\n",
    "\n",
    "We will use a dataset of simplified \"hand-written\" digits for classification into one of ten classes (0-9). The representation is in a text format (see below) to make it easy to handle. \n",
    "\n",
    "For this dataset the inputs will be a 5x5 matrix of binary \"pixels\" (0 or 1, represented pictorially as '.' or '1' for input and '.' or '@' for output).\n",
    "\n",
    "The network structure will be:\n",
    "\n",
    "25 inputs (pixels)\n",
    "\n",
    "5 hidden units\n",
    "\n",
    "10 output units.\n",
    "\n",
    "The output unit with the largest value will taken as the predicted digit.\n",
    "\n",
    "We will run the network for 10000 iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the raw digit input and the target value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_digits = [\n",
    "      \"\"\"11111\n",
    "         1...1\n",
    "         1...1\n",
    "         1...1\n",
    "         11111\"\"\",\n",
    "\n",
    "      \"\"\"..1..\n",
    "         ..1..\n",
    "         ..1..\n",
    "         ..1..\n",
    "         ..1..\"\"\",\n",
    "\n",
    "      \"\"\"11111\n",
    "         ....1\n",
    "         11111\n",
    "         1....\n",
    "         11111\"\"\",\n",
    "\n",
    "      \"\"\"11111\n",
    "         ....1\n",
    "         11111\n",
    "         ....1\n",
    "         11111\"\"\",\n",
    "\n",
    "      \"\"\"1...1\n",
    "         1...1\n",
    "         11111\n",
    "         ....1\n",
    "         ....1\"\"\",\n",
    "\n",
    "      \"\"\"11111\n",
    "         1....\n",
    "         11111\n",
    "         ....1\n",
    "         11111\"\"\",\n",
    "\n",
    "      \"\"\"11111\n",
    "         1....\n",
    "         11111\n",
    "         1...1\n",
    "         11111\"\"\",\n",
    "\n",
    "      \"\"\"11111\n",
    "         ....1\n",
    "         ....1\n",
    "         ....1\n",
    "         ....1\"\"\",\n",
    "\n",
    "      \"\"\"11111\n",
    "         1...1\n",
    "         11111\n",
    "         1...1\n",
    "         11111\"\"\",\n",
    "\n",
    "      \"\"\"11111\n",
    "         1...1\n",
    "         11111\n",
    "         ....1\n",
    "         11111\"\"\"]\n",
    "\n",
    "def make_digit(raw_digit):\n",
    "    return [1 if c == '1' else 0\n",
    "            for row in raw_digit.split(\"\\n\")\n",
    "            for c in row.strip()]\n",
    "\n",
    "inputs = np.array(list(map(make_digit, raw_digits)))\n",
    "\n",
    "targets = np.eye(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "Here is a Neural Network object, providing the ability to define the learning rate, number of epochs/iterations, batch size, the number of layers and the number of neurons in each layer. The default setting of learning_rate, epochs, batch size and neural_numbers are 0.1, 1000, None, and \\[10\\] respectively. If batch_size is set to be None, that means all samples will be used for training in each iteration. \\[10\\] means that there is only one hidden layer with 10 neurons. If you want to change the number of hidden layers or the number of neurons, you can change the value of ```neural_numbers```. \n",
    "\n",
    "Compare your function code from above with the ones used in this implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "    def __init__(self, learning_rate=0.1, epochs=1000, batch_size=None,neural_numbers=[10]):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.neural_numbers=neural_numbers\n",
    "        self.layers=len(self.neural_numbers)+1\n",
    "        np.random.seed(77)\n",
    "\n",
    "    def fit(self,X,y):\n",
    "        self.X,self.y = X,y\n",
    "        self.initial_weight()\n",
    "        self.backpropagate(X,y)\n",
    "    \n",
    "    def forward(self,X):\n",
    "        output_list = []\n",
    "        input_x = X\n",
    "\n",
    "        for layer in range(self.layers):\n",
    "\n",
    "            cur_weight = self.weight_list[layer]\n",
    "            cur_bias = self.bias_list[layer]\n",
    "            # Calculate the output for current layer\n",
    "            output = self.neuron_output(cur_weight,input_x,cur_bias)\n",
    "            # The current output will be the input for the next layer.\n",
    "            input_x =  output\n",
    "\n",
    "            output_list.append(output)\n",
    "        return output_list\n",
    "\n",
    "    def backpropagate(self,train_x,train_y):\n",
    "        acc_list=[]\n",
    "        for iteration in range(self.epochs):\n",
    "            if self.batch_size:\n",
    "                n=train_x.shape[0]\n",
    "                # Sample batch_size number of sample for n samples\n",
    "                sample_index=np.random.choice(n, self.batch_size, replace=False)\n",
    "                x=train_x[sample_index,:]\n",
    "                y=train_y[sample_index,:]\n",
    "            else:\n",
    "                x=train_x\n",
    "                y=train_y\n",
    "\n",
    "            output_list=self.forward(x)\n",
    "            y_pred=output_list.pop()\n",
    "            # Record the accuracy every 5 iteration.\n",
    "            if iteration%5==0:\n",
    "                acc=self.accuracy(self.softmax(y),self.softmax(y_pred))\n",
    "                acc_list.append(acc)\n",
    "\n",
    "            loss_last=y-y_pred\n",
    "\n",
    "            output=y_pred\n",
    "\n",
    "            for layer in range(self.layers-1,-1,-1):\n",
    "                if layer!=0:\n",
    "                    input_last=output_list.pop()\n",
    "                else:\n",
    "                    input_last=x\n",
    "\n",
    "                if layer==self.layers-1:\n",
    "                    loss,dw,db=self.der_last_layer(loss_last,output,input_last)\n",
    "                else:\n",
    "                    weight=self.weight_list[layer+1]\n",
    "                    loss,dw,db=self.der_hidden_layer(loss_last,output,input_last,weight)\n",
    "\n",
    "                output=input_last\n",
    "                self.weight_list[layer] +=dw*self.learning_rate\n",
    "                self.bias_list[layer] +=db*self.learning_rate\n",
    "                loss_last=loss\n",
    "        self.acc_list=acc_list\n",
    "\n",
    "    def predict(self,X):\n",
    "        output_list = self.forward(X)\n",
    "        pred_y = self.softmax(output_list[-1])\n",
    "        return pred_y\n",
    "\n",
    "    def accuracy(self, pred, y_test):\n",
    "        assert len(pred) == len(y_test)\n",
    "        true_pred=np.where(pred==y_test)\n",
    "        if true_pred:\n",
    "            true_n = true_pred[0].shape[0]\n",
    "            return true_n/len(pred)\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def initial_weight(self):\n",
    "        if self.X is not None and self.y is not None:\n",
    "            x=self.X\n",
    "            y=self.y\n",
    "            input_dim = x.shape[1]\n",
    "            output_dim = y.shape[1]\n",
    "\n",
    "            number_NN = self.neural_numbers+[output_dim]\n",
    "\n",
    "            weight_list,bias_list = [],[]\n",
    "            last_neural_number = input_dim     \n",
    "\n",
    "            for cur_neural_number in number_NN:\n",
    "                # The dimension of weight matrix is last neural number * current neural number\n",
    "                weights = np.random.randn(last_neural_number, cur_neural_number)\n",
    "                # The number of dimension for bias is 1 and the number of current neural\n",
    "                bias = np.zeros((1, cur_neural_number))\n",
    "\n",
    "                last_neural_number=cur_neural_number\n",
    "\n",
    "                weight_list.append(weights)\n",
    "                bias_list.append(bias)\n",
    "\n",
    "            self.weight_list=weight_list\n",
    "            self.bias_list=bias_list\n",
    "\n",
    "    # Classical sigmoid activation functions are used in every layer in this network\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    # Derivation of the sigmoid activation function\n",
    "    def sigmoid_der(self, x):\n",
    "        return (1 - x) * x\n",
    "\n",
    "    # Calculate the output for this layer\n",
    "    def neuron_output(self,w,x,b):\n",
    "        wx=np.dot(x, w)\n",
    "        return self.sigmoid( wx + b)\n",
    "\n",
    "    def der_last_layer(self,loss_last,output,input_x):\n",
    "        sigmoid_der=self.sigmoid_der(output)\n",
    "        loss = sigmoid_der*loss_last\n",
    "        dW = np.dot(input_x.T, loss)\n",
    "        db = np.sum(loss, axis=0, keepdims=True)\n",
    "        return loss,dW,db\n",
    "\n",
    "    def der_hidden_layer(self,loss_last,output,input_x,weight):\n",
    "        loss = self.sigmoid_der(output) * np.dot(loss_last,weight.T)\n",
    "        db = np.sum(loss, axis=0, keepdims=True)\n",
    "        dW = np.dot(input_x.T, loss)\n",
    "        return loss,dW,db\n",
    "\n",
    "    def softmax(self,y):\n",
    "        return np.argmax(y,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to run the implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Learning_rate=0.05\n",
    "nn=NeuralNetwork(learning_rate=Learning_rate)\n",
    "nn.fit(inputs,targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimenting with the implementation\n",
    "\n",
    "Parameter turning is not that easy in Neural Networks. To see this, let's investigate the relationship between learning rate and accuracy. Below is a function to test the effect of learning rate on accuracy. Run it and it should generate some plots to show the effect.\n",
    "\n",
    "If you want to try other values for the learning rate, or investigate the effect of other parameters, go ahead and change them and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_LearnRate(Learning_rate,inputs,targets):\n",
    "    nn=NeuralNetwork(learning_rate=Learning_rate)\n",
    "    nn.fit(inputs,targets)\n",
    "    acc_array=np.array(nn.acc_list)\n",
    "    plt.plot(np.arange(acc_array.shape[0])*5,acc_array)\n",
    "    plt.title(\"Learning Rate:{}\".format(Learning_rate))\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.xlabel(\"Number of iterations\")\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.subplot(2,2,1)\n",
    "Learning_rate=0.05\n",
    "test_LearnRate(Learning_rate,inputs,targets)\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "Learning_rate=0.1\n",
    "test_LearnRate(Learning_rate,inputs,targets)\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "Learning_rate=0.5\n",
    "test_LearnRate(Learning_rate,inputs,targets)\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "Learning_rate=1\n",
    "test_LearnRate(Learning_rate,inputs,targets)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
